import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.linear_model import *
import statsmodels.api as sm
import billboard
import datetime
import warnings
warnings.filterwarnings("ignore")

def load_and_combine_csv_files():
    """Load and combine all Taylor Swift Spotify data files"""
    file_names = [
        "/Users/Projects/taylor swift /datasets/Taylor Swift Spotify Data 07-23.csv",
        "/Users/Projects/taylor swift /datasets/Taylor Swift Spotify Data 11-24-2024.csv",
        "/Users/Projects/taylor swift /datasets/Taylor Swift Spotify Data 1102.csv",
        "/Users/Projects/taylor swift /datasets/Taylor Swift Spotify Data Up1.csv",
        "/Users/Projects/taylor swift /datasets/Taylor Swift Spotify Data Up2.csv",
        "/Users/Projects/taylor swift /datasets/Taylor Swift Spotify Data.csv"
    ]
    
    dataframes = []
    for file in file_names:
        try:
            df = pd.read_csv(file)
            dataframes.append(df)
            print(f"Successfully loaded {file}")
        except FileNotFoundError:
            print(f"Warning: Could not find {file}")
            continue
    
    # Combine all dataframes
    if dataframes:
        combined_df = pd.concat(dataframes, ignore_index=True)
        # Remove duplicates based on track name and album name
        combined_df = combined_df.drop_duplicates(subset=['album_name', 'track_name'], keep='first')
        print(f"Combined data shape: {combined_df.shape}")
        return combined_df
    else:
        raise Exception("No data files could be loaded!")

def clean_spotify_data(data):
    """Clean the Spotify dataset by removing unwanted versions and duplicates"""
    # Remove unwanted album versions
    album_filters = ['Karaoke', 'Radio', 'International', 'Disney', 'Japanese', 
                    'Tour', 'Package', 'Platinum', 'US Version', 'Live']
    for filter in album_filters:
        data = data.drop(data[data['album_name'].str.contains(filter, na=False)].index)
    
    # Remove unwanted song versions
    song_filters = ["Karaoke", "Radio", "Mix", "10 Minute Version"]
    for filter in song_filters:
        data = data.drop(data[data['track_name'].str.contains(filter, na=False)].index)
    
    # Fix album dates
    data = data.drop(data[data['album_release_date'] == "2014-01-01"].index)
    data = data.drop(data[data['album_release_date'] == "2008-01-01"].index)
    
    return data

def get_billboard_data(song_name, release_date, threshold=20):
    """Collect Billboard Hot 100 data for a given song"""
    data = {"weeks": 0, "peak": 0}
    weeks_missing = 0
    date_itr = release_date
    today = datetime.datetime.now()
    saved_charts = {}
    
    while date_itr <= today and weeks_missing < threshold:
        date_string = date_itr.strftime("%Y-%m-%d")
        
        try:
            if date_string in saved_charts:
                chart = saved_charts[date_string]
            else:
                chart = billboard.ChartData('hot-100', date=date_string, fetch=True, timeout=25)
                saved_charts[date_string] = chart
                
            for song in chart:
                if song.title.strip().lower().replace('’','\'') in song_name.strip().lower().replace('’','\'') and "Taylor Swift" in song.artist:
                    data["weeks"] = song.weeks
                    data["peak"] = song.peakPos
                    weeks_missing = 0
                    
            weeks_missing += 1
            date_itr = date_itr + datetime.timedelta(days=7)
        except Exception as e:
            print(f"Error fetching Billboard data for {song_name}: {str(e)}")
            break
    
    return data

def combine_data(spotify_data):
    """Add Billboard data to Spotify dataset"""
    weeks_on_chart = []
    peak_on_chart = []
    
    print("Collecting Billboard data for each song...")
    total_songs = len(spotify_data)
    
    for index, row in spotify_data.iterrows():
        print(f"Processing {index + 1}/{total_songs}: {row['track_name']} from {row['album_name']}")
        try:
            release_date = datetime.datetime.strptime(row["album_release_date"], "%Y-%m-%d")
            billboard_data = get_billboard_data(row["track_name"], release_date)
            weeks_on_chart.append(billboard_data["weeks"])
            peak_on_chart.append(billboard_data["peak"])
        except Exception as e:
            print(f"Error processing {row['track_name']}: {str(e)}")
            weeks_on_chart.append(0)
            peak_on_chart.append(0)
    
    spotify_data["weeks"] = weeks_on_chart
    spotify_data["peak"] = peak_on_chart
    
    return spotify_data

def create_basic_visualizations(data):
    """Create basic visualizations of release patterns and chart performance"""
    # Songs per year
    plt.figure(figsize=(12, 6))
    sns.countplot(x="album_release_year", data=data, palette=sns.color_palette("flare", 10))
    plt.title("Songs Released By Year")
    plt.xlabel("Year")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.show()
    
    # Hot 100 entries by album
    charted = data[data["peak"] > 0]
    charted_by_album = charted.groupby("album_name", as_index=False)
    
    plt.figure(figsize=(15, 6))
    ax = charted_by_album.agg({'peak': 'count'}).plot.bar(
        x="album_name", 
        y="peak",
        title="Number of Hot 100 Songs by Album",
        color="lightblue"
    )
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

def analyze_acoustic_features(data):
    """Create distribution plots for acoustic features"""
    features = ["danceability", "energy", "acousticness", "valence", 
                "tempo", "loudness"]
    
    fig, axes = plt.subplots(3, 2, figsize=(15, 20))
    axes = axes.ravel()
    
    colors = sns.color_palette("flare", len(features))
    
    for idx, feature in enumerate(features):
        sns.histplot(data=data, x=feature, ax=axes[idx], color=colors[idx])
        axes[idx].set_title(f"Distribution of {feature}")
    
    plt.tight_layout()
    plt.show()

def analyze_correlations(data):
    """Create correlation plots between features and chart performance"""
    features = ["danceability", "energy", "acousticness", "valence", 
                "tempo", "loudness"]
    
    # For all charted songs
    charted = data[data["peak"] > 0]
    fig, axes = plt.subplots(3, 2, figsize=(15, 20))
    axes = axes.ravel()
    
    for idx, feature in enumerate(features):
        sns.regplot(data=charted, x=feature, y="peak", ax=axes[idx], 
                   color=sns.color_palette("husl", 6)[idx])
        axes[idx].set_title(f"Peak Position vs {feature}")
    
    fig.suptitle("Acoustic Metrics vs. Peak Position on Billboard Hot 100", fontsize=16)
    plt.tight_layout()
    plt.show()
    
    # For songs with 5+ weeks on chart
    weeks_5 = data[data["weeks"] >= 5]
    fig, axes = plt.subplots(3, 2, figsize=(15, 20))
    axes = axes.ravel()
    
    for idx, feature in enumerate(features):
        sns.regplot(data=weeks_5, x=feature, y="weeks", ax=axes[idx],
                   color=sns.color_palette("husl", 6)[idx])
        axes[idx].set_title(f"Weeks on Chart vs {feature} (5+ weeks)")
    
    fig.suptitle("Acoustic Metrics vs. Weeks on Billboard Hot 100 (5+ weeks)", fontsize=16)
    plt.tight_layout()
    plt.show()

def main():
    print("Starting Taylor Swift Music Analysis...")
    
    # Load and combine all CSV files
    print("Loading and combining CSV files...")
    combined_df = load_and_combine_csv_files()
    
    # Clean Spotify data
    print("Cleaning Spotify data...")
    spotify_data = clean_spotify_data(combined_df)
    
    # Add Billboard data
    print("Adding Billboard data...")
    combined_data = combine_data(spotify_data)
    
    # Save combined data
    combined_data.to_csv("taylor_swift_complete_analysis.csv", index=False)
    print("Combined data saved to taylor_swift_complete_analysis.csv")
    
    # Create visualizations
    print("Creating visualizations...")
    create_basic_visualizations(combined_data)
    
    # Analyze acoustic features
    print("Analyzing acoustic features...")
    analyze_acoustic_features(combined_data)
    
    # Analyze correlations
    print("Analyzing correlations...")
    analyze_correlations(combined_data)
    import matplotlib.pyplot as plt
import os

# Directory to save the PNG files
save_dir = '/Users/Projects/taylor swift'

# Ensure the directory exists
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# Sample code to generate and save multiple plots
for i in range(1, 6):  # Example: saving 5 plots
    plt.figure()  # Create a new figure for each plot
    plt.plot([1, 2, 3], [i, i+1, i+2])  # Sample plot; replace with your own plotting logic
    plot_filename = f"plot_{i}.png"  # Generate a unique filename for each plot
    plot_path = os.path.join(save_dir, plot_filename)
    plt.savefig(plot_path)  # Save the plot to the specified directory
    print(f"Saved {plot_filename} to {save_dir}")
    plt.close()  # Close the plot to avoid overlap with the next plot

    print("Analysis complete!")
   


if __name__ == "__main__":
    main()
